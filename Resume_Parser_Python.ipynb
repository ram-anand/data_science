{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume parser utlity \n",
    "\n",
    "It can multipage resumes in process word (.doc & .docx) and pdf (.pdf) files\n",
    "\n",
    "### Web Instance:\n",
    "\n",
    "1. Resumes can be uploaded to local url\n",
    "home url: http://127.0.0.1:5001\n",
    "2. On submit , it redirects to output page with parsed resume data in form of table\n",
    "output url: http://127.0.0.1:5001/output\n",
    "3. It also generates two extract file of parsed tabular data (RESUME_DUMP.CSV, RESUME_DUMP.TXT)\n",
    "\n",
    "Note: It can not process old word (.doc) file\n",
    "\n",
    "### Local Instance\n",
    "\n",
    "1. Resumes are placed in the directory indicated by constant \"RESUME_FOLDER\"\n",
    "2. To enable local instance, uncomment local instance main method in below code (also disable the web instance main method at the same time)\n",
    "3. Run the jupyter notebook to run resume parser\n",
    "4. It also generates two extract file of parsed tabular data (RESUME_DUMP.CSV, RESUME_DUMP.TXT)\n",
    "\n",
    "Note: It can process word (.doc & .docx) and pdf (.pdf) files. To process (.doc) file, it first converts them into (.docx) file.\n",
    "\n",
    "### Library Dependency\n",
    "1. pdfminer - to read (.pdf) file\n",
    "2. docx2txt -  to read (.docx) file\n",
    "3. spacy/nltk - for pattern matching and entity recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import docx2txt\n",
    "from difflib import SequenceMatcher\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from glob import glob\n",
    "import win32com.client as win32\n",
    "from win32com.client import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE CONFIGURATIONS\n",
    "\n",
    "# FILES UPLOADED TO FOLDER\n",
    "#UPLOAD_FOLDER = \"D:\\DATA_SCIENCE\\RESUME_PARSER\\RESUME\"\n",
    "UPLOAD_FOLDER = \"D:/DATA_SCIENCE/RESUME_PARSER/RESUME\"\n",
    "# FILES PLACED FOR LOCAL PARSING (USES DOUBLE BACK SLASH)\n",
    "RESUME_FOLDER = \"D:/DATA_SCIENCE/RESUME_PARSER/RESUME\"\n",
    "#RESUME_FOLDER = \"D:\\\\DATA_SCIENCE\\\\RESUME_PARSER\\\\RESUME\\\\\"\n",
    "\n",
    "# Education degree list\n",
    "EDUCATION_DB = \"academic_degree_db.csv\"\n",
    "\n",
    "# External csv file with list of skills\n",
    "SKILLS_DB = \"skills_db.csv\"\n",
    "\n",
    "# External csv file with list of indian surnames\n",
    "INDIAN_SURNAMES_DB = \"indian_surname_db.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN CONFIGURATIONS\n",
    "\n",
    "# NAME\n",
    "NAME_PATTERN      = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# PHONE NUMBER\n",
    "#PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "PHONE_REG = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "\n",
    "NOT_ALPHA_NUMERIC = r'[^a-zA-Z\\d]'\n",
    "\n",
    "NUMBER            = r'\\d+'\n",
    "\n",
    "# For finding date ranges\n",
    "MONTHS_SHORT      = r'(jan)|(feb)|(mar)|(apr)|(may)|(jun)|(jul)|(aug)|(sep)|(oct)|(nov)|(dec)'\n",
    "MONTHS_LONG       = r'(january)|(february)|(march)|(april)|(may)|(june)|(july)|(august)|(september)|(october)|(november)|(december)'\n",
    "MONTH             = r'(' + MONTHS_SHORT + r'|' + MONTHS_LONG + r')'\n",
    "YEAR              = r'(((20|19)(\\d{2})))'\n",
    "\n",
    "STOPWORDS         = set(stopwords.words('english'))\n",
    "\n",
    "RESUME_SECTIONS = {\n",
    "                    'education':['education','qualifications','academic','scholastic', 'certification'],\n",
    "                    'skills':['skills','competencies','technical','technologies','skillset','training','proficiencies'],\n",
    "                    'experience':['experience', 'professional experience','employment','job history','employment history','work experience','work history','professional experience','professional background']\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_docx(path):\n",
    "    # Create list of paths to .doc files\n",
    "    #paths = glob(file_path+\"//*.doc\", recursive=True)\n",
    "    # Opening MS Word\n",
    "    word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "    doc = word.Documents.Open(path)\n",
    "    doc.Activate ()\n",
    "    # Rename path with .docx\n",
    "    new_file_abs = os.path.abspath(path)\n",
    "    new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "    # Save and Close\n",
    "    word.ActiveDocument.SaveAs(\n",
    "        new_file_abs, FileFormat=constants.wdFormatXMLDocument\n",
    "    )\n",
    "    doc.Close(False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    '''\n",
    "    Extract text data from a pdf file\n",
    "    input: pdf file path\n",
    "    output: extracted text\n",
    "    '''\n",
    "    #print(\"Reading pdf document: \", pdf_path)\n",
    "    \n",
    "    output_string = io.StringIO()\n",
    "    with open(pdf_path, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "        device.close()\n",
    "\n",
    "    yield(output_string.getvalue())\n",
    "    \n",
    "#     with open(pdf_path, 'rb') as fh:\n",
    "#         for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "#             resource_manager = PDFResourceManager()\n",
    "#             fake_file_handle = io.StringIO()\n",
    "#             converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "#             page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "#             page_interpreter.process_page(page)\n",
    " \n",
    "#             text = fake_file_handle.getvalue()\n",
    "#             yield text\n",
    " \n",
    "#             # close open handles\n",
    "#             converter.close()\n",
    "#             fake_file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_doc(docx_path):\n",
    "    '''\n",
    "    Extract text data from a pdf file\n",
    "    input: docx file path\n",
    "    output: extracted text\n",
    "    '''\n",
    "    #print(\"Reading word document: \", docx_path)\n",
    "\n",
    "    temp = docx2txt.process(docx_path)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path, extension):\n",
    "    '''\n",
    "    call different function based on file type to extract text\n",
    "    '''\n",
    "    text = ''\n",
    "    if extension == '.pdf':\n",
    "        for page in extract_text_from_pdf(file_path):\n",
    "            text += ' ' + page\n",
    "    elif extension == '.docx':\n",
    "        text = extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        pass\n",
    "    text=str(text)\n",
    "    if \"CV\" in text:\n",
    "        text = text.replace(\"CV\",\"\")\n",
    "    if \"RESUME\" in text:\n",
    "        text = text.replace(\"RESUME\",\"\")\n",
    "    if \"Resume\" in text:\n",
    "        text = text.replace(\"Resume\",\"\")      \n",
    "    if \"curriculum vitae\" in text:\n",
    "        text = text.replace(\"curriculum vitae\",\"\")\n",
    "    if \"Curriculum Vitae\" in text:\n",
    "        text = text.replace(\"Curriculum Vitae\",\"\")    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resume_sections(text):\n",
    "    '''\n",
    "    extracts various sections, find start and end index of sections in resume text\n",
    "    '''\n",
    "    #text_split = [i.strip() for i in text.split('\\n')]\n",
    "    # sections_in_resume = [i for i in text_split if i.lower() in sections]\n",
    "    section_labels = RESUME_SECTIONS\n",
    "    end = len(text)-1\n",
    "    sections={}\n",
    "    sections['education']=[0,end]\n",
    "    sections['skills']=[0,end]\n",
    "    sections['experience']=[0,end]\n",
    "    \n",
    "    for value in section_labels.get('education'):\n",
    "        start = text.find(value.upper())\n",
    "        if start ==-1:\n",
    "            start = text.find(value.title())\n",
    "        if start != -1:\n",
    "            sections['education']=[start,end]\n",
    "    for value in section_labels.get('skills'):\n",
    "        start = text.find(value.upper())\n",
    "        if start ==-1:\n",
    "            start = text.find(value.title())        \n",
    "        if start != -1:\n",
    "            sections['skills']=[start,end]\n",
    "    for value in section_labels.get('experience'):\n",
    "        start = text.find(value.upper())\n",
    "        if start ==-1:\n",
    "            start = text.find(value.title())\n",
    "        if start != -1:\n",
    "            sections['experience']=[start,end] \n",
    "\n",
    "    # update education index\n",
    "    if sections.get('education')[0]<sections.get('skills')[0]:\n",
    "        sections['education'][1] = sections.get('skills')[0]\n",
    "        if (sections.get('education')[1]>sections.get('experience')[0]) and (sections.get('education')[0]<sections.get('experience')[0]):\n",
    "            sections['education'][1] = sections.get('experience')[0]\n",
    "            \n",
    "    # update skills index\n",
    "    if sections.get('skills')[0]<sections.get('education')[0]:\n",
    "        sections['skills'][1] = sections.get('education')[0]\n",
    "        if (sections.get('skills')[1]>sections.get('experience')[0]) and (sections.get('skills')[0]<sections.get('experience')[0]):\n",
    "            sections['skills'][1] = sections.get('experience')[0]    \n",
    "        \n",
    "    # update experience index\n",
    "    if sections.get('experience')[0]<sections.get('education')[0]:\n",
    "        sections['experience'][1] = sections.get('education')[0]\n",
    "        if (sections.get('experience')[1]>sections.get('skills')[0]) and (sections.get('experience')[0]<sections.get('skills')[0]):\n",
    "            sections['experience'][1] = sections.get('skills')[0]   \n",
    "            \n",
    "    #print(sections)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(nlp_text, matcher, text, email):\n",
    "    '''\n",
    "    extract name from spacy nlp text and/or raw text \n",
    "    '''\n",
    "    data = pd.read_csv(INDIAN_SURNAMES_DB)\n",
    "    surname_bank = list(data.surname.values)\n",
    "    start_name = \"\"  \n",
    "\n",
    "    # extracting name from resume text which partially matched with email id\n",
    "    text = re.sub(\"\\S+@\\S+(?:\\.\\S+)+\", '', text)\n",
    "    text = re.sub('[^A-Za-z\\s]+', '', text).lower().split()\n",
    "    wordlist = [word for word in text if word not in STOPWORDS]\n",
    "    start_name = wordlist[0]+ \" \" + wordlist[1]\n",
    "    \n",
    "    if email:\n",
    "        for i, word in enumerate(wordlist):\n",
    "            try:\n",
    "                first_name = word\n",
    "                last_name = wordlist[i+1]\n",
    "                name = first_name + \" \" + last_name\n",
    "                if word:\n",
    "                    if (SequenceMatcher(None, first_name, email).ratio()>0.3):\n",
    "                        return (name)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # extracting name from resume text that matches on the surnames list \n",
    "    for i, word in enumerate(wordlist):\n",
    "        try:\n",
    "            first_name = word\n",
    "            last_name = wordlist[i+1]\n",
    "            name = first_name + \" \" + last_name\n",
    "            if (last_name in surname_bank) or (first_name in surname_bank):\n",
    "                return name\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # extracting name using pattern match\n",
    "    pattern = [NAME_PATTERN]\n",
    "    matcher.add('NAME', None, *pattern)\n",
    "    matches = matcher(nlp_text)\n",
    "    if email:\n",
    "        for match_id, start, end in matches:\n",
    "            name = nlp_text[start:end].text\n",
    "            if name:\n",
    "                if (SequenceMatcher(None, name, email).ratio()>0.3):\n",
    "                    return name\n",
    "\n",
    "    return start_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mobile_number(text):\n",
    "    '''\n",
    "    extract mobile number from text\n",
    "    '''\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    #phone = re.findall(re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'))\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(text):\n",
    "    '''\n",
    "    Retrieve email id from text\n",
    "    '''\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text.lower())\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_education(nlp_text):\n",
    "    '''\n",
    "    Extract education from spacy nlp text, output is tuple of education degree and/or year if year if found\n",
    "    '''\n",
    "    edu = {}\n",
    "    flag = False\n",
    "    degree_bank = pd.read_csv(EDUCATION_DB, index_col=0)\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        text = text.replace(\"!@#$%^&*()[]{};:,/<>?\\|`~-=_+\", \"\")\n",
    "        \n",
    "        for i in degree_bank.index:\n",
    "            degree = str(degree_bank.iloc[i,0])\n",
    "            if degree in text.upper():\n",
    "                try:\n",
    "                    edu[degree] = text + nlp_text[index + 1]\n",
    "                    flag = True\n",
    "                    break\n",
    "                except:\n",
    "                    edu[degree] = text\n",
    "                    flag = True\n",
    "                    break                \n",
    "        if flag:\n",
    "            break\n",
    "            \n",
    "#         for word in text.split():\n",
    "#             #word = re.sub(r'[?|$|.|!|,]', r'', word)\n",
    "#             if (word not in STOPWORDS):\n",
    "#                 for degree in EDUCATION:\n",
    "#                     if degree.find(word.upper()) != -1:\n",
    "#                         try:\n",
    "#                             edu[degree] = text + nlp_text[index + 1]\n",
    "#                             flag = True\n",
    "#                             break\n",
    "#                         except:\n",
    "#                             edu[degree] = text\n",
    "#                             flag = True\n",
    "#                             break\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(YEAR), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year.group(0))))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(nlp_text, noun_chunks):\n",
    "    '''\n",
    "    extract skills from spacy nlp text, ouput is a list of skills\n",
    "    '''\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    data = pd.read_csv(SKILLS_DB)\n",
    "    skills_bank = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills_bank:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams\n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills_bank:\n",
    "            skillset.append(token)\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeParser(object):\n",
    "    def __init__(self, resume):\n",
    "        #self.dict.\n",
    "        #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nlp = en_core_web_sm.load()\n",
    "        self.__matcher = Matcher(nlp.vocab)\n",
    "        self.__details = {\n",
    "            'name'              : None,\n",
    "            'first_name'        : None,\n",
    "            'last_name'         : None,\n",
    "            'email'             : None,\n",
    "            'mobile_number'     : None,\n",
    "            'skills'            : None,\n",
    "            'education'         : None\n",
    "        }\n",
    "        self.__resume      = resume\n",
    "        self.__text_raw    = extract_text(self.__resume, os.path.splitext(self.__resume)[1])\n",
    "        self.__text        = ' '.join(self.__text_raw.split()).replace(\"!#$%^&*()[]{}/<>?\\|`~-=+\", \" \").replace('\\u200b', ' ').strip()\n",
    "        self.__sections    = extract_resume_sections(self.__text)\n",
    "        self.__nlp         = nlp(re.sub('[^A-Za-z\\s,;:]+', '', self.__text).lower())\n",
    "        self.__nlp_edu     = nlp(self.__text[self.__sections[\"education\"][0]:self.__sections[\"education\"][1]])\n",
    "        self.__noun_chunks = list(self.__nlp.noun_chunks)\n",
    "        self.__get_basic_details()\n",
    "\n",
    "    def get_extracted_data(self):\n",
    "        return self.__details\n",
    "\n",
    "    def __get_basic_details(self):\n",
    "        email      = extract_email(self.__text)\n",
    "        mobile     = extract_mobile_number(self.__text)\n",
    "        #edu        = extract_education([sent.string.strip() for sent in self.__nlp_edu.sents])\n",
    "        edu        = extract_education([sent.string.strip() for sent in self.__nlp.sents])\n",
    "        skills     = extract_skills(self.__nlp, self.__noun_chunks)\n",
    "        name       = extract_name(self.__nlp, matcher=self.__matcher, text=self.__text, email=email)\n",
    "        if \" \" in name:\n",
    "            first_name = name.split(\" \")[:-1]\n",
    "            last_name  = name.split(\" \")[-1]\n",
    "        else:\n",
    "            first_name = name\n",
    "            last_name = \"\"\n",
    "        self.__details['name'] = \" \".join(name) if type(name) is list else name\n",
    "        self.__details['first_name'] = \" \".join(first_name) if type(first_name) is list else first_name\n",
    "        self.__details['last_name'] = last_name\n",
    "        self.__details['email'] = email\n",
    "        self.__details['mobile_number'] = mobile\n",
    "        self.__details['education'] = ' '.join([str(elem) for elem in edu])\n",
    "        self.__details['skills'] = \", \".join(skills) if type(skills) is list else skills\n",
    "        #print(self.__sections)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_docx(file_path):\n",
    "    # doc_files = glob(file_path+'\\\\*.doc', recursive=True)\n",
    "\n",
    "    # Opening MS Word\n",
    "    word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "    doc = word.Documents.Open(file_path)\n",
    "    doc.Activate ()\n",
    "\n",
    "    # Rename path with .docx\n",
    "    new_file_abs = os.path.abspath(file_path)\n",
    "    new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "\n",
    "    # Save and Close\n",
    "    word.ActiveDocument.SaveAs(\n",
    "        new_file_abs, FileFormat=constants.wdFormatXMLDocument\n",
    "    )\n",
    "    doc.Close(False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_convert_doc_to_docx(file_path):\n",
    "    try:\n",
    "        doc_files = glob(file_path+'\\\\*.doc', recursive=True)\n",
    "        for doc_path in doc_files:\n",
    "            print(\"File to convert: \", str(doc_path))\n",
    "            convert_doc_to_docx(doc_path)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume_details(file_path):\n",
    "    resumes = []\n",
    "    #print(\"current Path\", str(file_path))\n",
    "    bulk_convert_doc_to_docx(file_path)\n",
    "    # Glob module matches certain patterns\n",
    "    docx_files = glob(file_path+\"\\\\*.docx\")\n",
    "    pdf_files = glob(file_path+\"\\\\*.pdf\")\n",
    "    rtf_files = glob(file_path+\"\\\\*.rtf\")\n",
    "    text_files = glob(file_path+\"\\\\*.txt\")\n",
    "\n",
    "    files = set(docx_files + pdf_files + rtf_files + text_files)\n",
    "    resumes = list(files)\n",
    "    #print (\"%d files identified\" %len(files))\n",
    "    resume_records = [ResumeParser(x).get_extracted_data() for x in resumes]\n",
    "    #print(resumes)\n",
    "\n",
    "    #df = pd.DataFrame(columns=[\"name\", \"first_name\", \"last_name\", \"email\", \"mobile_number\", \"education\", \"skills\"])\n",
    "    df = pd.DataFrame(columns=[\"first_name\", \"last_name\", \"email\", \"mobile_number\", \"education\", \"skills\"])\n",
    "    for i, resume in enumerate(resume_records):\n",
    "        #print(pd.DataFrame(resume))\n",
    "        #df.append(pd.DataFrame(resume), ignore_index = True)\n",
    "        #df.loc[i,'name'] = resume.get('name', \"\")\n",
    "        df.loc[i,'first_name'] = resume.get('first_name', \"\")\n",
    "        df.loc[i,'last_name'] = resume.get('last_name', \"\")\n",
    "        df.loc[i,'email'] = resume.get('email',\"\")\n",
    "        df.loc[i,'mobile_number'] = resume.get('mobile_number',\"\")\n",
    "        df.loc[i,'education'] = resume.get('education',\"\")\n",
    "        df.loc[i,'skills'] = resume.get('skills',\"\")\n",
    "        \n",
    "    subdir = os.path.join(file_path, \"OUTPUT\")\n",
    "    #print(\"Output directory:\", subdir)\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir)\n",
    "    \n",
    "    df.to_csv(os.path.join(subdir, \"RESUME_DUMP.CSV\"), index=False)\n",
    "    file1 = open(os.path.join(subdir, \"RESUME_DUMP.TXT\"),\"w\")\n",
    "    #file1.write(\"\\n\")\n",
    "    for line in resume_records:\n",
    "        file1.writelines(str(json.dumps(line))+\"\\n\")\n",
    "        #file1.writelines(str(line.items())+\"\\n\")\n",
    "    file1.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import magic\n",
    "import urllib.request\n",
    "#from app import app\n",
    "from flask import Flask, flash, request, redirect, render_template\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "ALLOWED_EXTENSIONS = set(['doc', 'docx', 'pdf'])\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = \"secret key\"\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "dir = os.path.join(UPLOAD_FOLDER)\n",
    "#print(\"Output directory:\", subdir)\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "#app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "@app.route('/')\n",
    "def upload_form():\n",
    "    return render_template('upload_resume.html')\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        # check if the post request has the file part\n",
    "        if 'files[]' not in request.files:\n",
    "            flash('No file part')\n",
    "            return redirect(request.url)\n",
    "        \n",
    "        files = request.files.getlist('files[]')\n",
    "\n",
    "        for file in files:\n",
    "            if file.filename == '':\n",
    "                flash('No file selected for uploading')\n",
    "                return redirect(request.url)\n",
    "            if file and allowed_file(file.filename):\n",
    "                filename = secure_filename(file.filename)\n",
    "                file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "                flash('File(s) successfully uploaded')\n",
    "                #if resp.status_code = 200:\n",
    "            else:\n",
    "                flash('Allowed file types are word and pdf')\n",
    "                return redirect(request.url)\n",
    "        return redirect('/output')\n",
    "\n",
    "@app.route('/output')\n",
    "def output():\n",
    "    df = get_resume_details(RESUME_FOLDER)\n",
    "    #print(df.head())\n",
    "    #return render_template('home_resume.html', output_data=df.to_dict(orient='records'))\n",
    "    return render_template('output_resume.html', tables=[df.to_html(classes='data', header=\"true\", index=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5001/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [30/Mar/2021 21:40:07] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [30/Mar/2021 21:40:13] \"\u001b[32mPOST / HTTP/1.1\u001b[0m\" 302 -\n",
      "127.0.0.1 - - [30/Mar/2021 21:40:15] \"\u001b[37mGET /output HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# FOR WEB BASED RESUME PARSING\n",
    "# Displays parsed resume data on webpage, also generates \"RESUME_DUMP.csv\" and \"RESUME_DUMP.txt\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bulk_convert_doc_to_docx(RESUME_FOLDER)\n",
    "    app.run(port=5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FOR LOCAL RESUME PARSING\n",
    "# Generates output \"RESUME_REPORT.csv\" and \"RESUME_REPORT.txt\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     bulk_convert_doc_to_docx(RESUME_FOLDER)\n",
    "#     get_resume_details(RESUME_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
